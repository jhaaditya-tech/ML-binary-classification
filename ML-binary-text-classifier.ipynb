{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWhyQIHKVpxb"
      },
      "source": [
        "# Downloading the training data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi80xp0QmzHB",
        "outputId": "a4fbc1d8-e119-4a7b-885a-e7a882efb202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnMT-wzbVpxd"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46iovl0KVpxe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Group 1: Technology & Science\n",
        "group_1 = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
        "           'comp.windows.x', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space']\n",
        "\n",
        "# Group 2: Sports, Politics, & Miscellaneous\n",
        "group_2 = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey',\n",
        "           'talk.politics.misc', 'talk.politics.guns', 'talk.politics.mideast', 'talk.religion.misc',\n",
        "           'misc.forsale', 'alt.atheism', 'soc.religion.christian']\n",
        "\n",
        "# Fetch the 20 Newsgroups training dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=group_1 + group_2, remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e3VQ1mPVpxe",
        "outputId": "ed06bc63-6790-4313-92ab-f74798ea47a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training data points: 11314\n",
            "First 500 class labels: [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "# Relabel the target: 0 for Group 1, 1 for Group 2\n",
        "group_1_labels = [newsgroups_train.target_names.index(cat) for cat in group_1]\n",
        "group_2_labels = [newsgroups_train.target_names.index(cat) for cat in group_2]\n",
        "\n",
        "binary_labels = [0 if label in group_1_labels else 1 for label in newsgroups_train.target]\n",
        "\n",
        "print(f'Number of training data points: {len(newsgroups_train.data)}')\n",
        "print(f'First 500 class labels: {binary_labels[:500]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G6mfQHtVpxf"
      },
      "source": [
        "# Preprocessing Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "buVYFga7Vpxf",
        "outputId": "65d6c714-1f55-4b92-bc42-67e67137325e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf4QnsoIVpxf"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns04IZGMVpxf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import contractions\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, labels, threshold =0.5):\n",
        "        self.stopwords = set(stopwords.words('english'))\n",
        "        self.vocabulary = {}  # Use a dictionary for fast lookups {word: index}\n",
        "        self.gini_scores = {}\n",
        "        self.labels = labels\n",
        "        self.stemmer = PorterStemmer()  # Initialize the Porter Stemmer\n",
        "        self.word_counts = defaultdict(int)  # Track the number of documents containing each word\n",
        "        self.threshold = threshold  # Gini index threshold for filtering\n",
        "\n",
        "    def preprocess(self, tokens):\n",
        "        \"\"\"Preprocesses tokens by expanding contractions, converting to lowercase, removing stopwords, applying stemming, etc.\"\"\"\n",
        "        cleaned_tokens = []\n",
        "        for token in tokens:\n",
        "            # Expand contractions\n",
        "            token = contractions.fix(token)\n",
        "\n",
        "            # Remove URLs and emails\n",
        "            token = re.sub(r'http\\S+|www\\S+|https\\S+', '', token, flags=re.MULTILINE)\n",
        "            token = re.sub(r'\\S+@\\S+', '', token)\n",
        "\n",
        "            # Remove @mentions and hashtags\n",
        "            token = re.sub(r'@\\w+', '', token)\n",
        "            token = re.sub(r'#\\w+', '', token)\n",
        "\n",
        "            # Remove non-alphanumeric characters and lowercase\n",
        "            cleaned_token = re.sub(r'[^a-z0-9]', '', token.lower())\n",
        "\n",
        "            # Remove stopwords, short words, and digits\n",
        "            if cleaned_token and cleaned_token not in self.stopwords and len(cleaned_token) > 2 and not cleaned_token.isdigit():\n",
        "                cleaned_token = self.stemmer.stem(cleaned_token)\n",
        "                cleaned_tokens.append(cleaned_token)\n",
        "\n",
        "        return cleaned_tokens\n",
        "\n",
        "\n",
        "    def build_vocabulary(self, preprocessed_docs):\n",
        "        \"\"\"Builds a vocabulary from preprocessed documents.\"\"\"\n",
        "        word_counts = defaultdict(int)\n",
        "\n",
        "        for doc in preprocessed_docs:\n",
        "            for word in doc:\n",
        "                word_counts[word] += 1\n",
        "\n",
        "        self.word_counts = word_counts\n",
        "\n",
        "        # original vocabulary without filtering\n",
        "        self.vocabulary = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
        "\n",
        "        print(f\"Vocabulary: {len(self.vocabulary)} words\")\n",
        "\n",
        "        # Store the filtered vocabulary and word counts\n",
        "        self.vocabulary = {word: idx for idx, word in enumerate(word for word, count in word_counts.items())}\n",
        "        self.word_counts = word_counts\n",
        "\n",
        "\n",
        "        return self.vocabulary\n",
        "\n",
        "\n",
        "    def calculate_gini_and_filter(self, docs):\n",
        "        \"\"\"\n",
        "        Calculates Gini index for words and filters the vocabulary.\n",
        "        \"\"\"\n",
        "        # Identify the unique class labels\n",
        "        classes = np.unique(self.labels)\n",
        "        num_classes = len(classes)\n",
        "\n",
        "        # Initialize word counts for each class\n",
        "        word_counts = defaultdict(lambda: np.zeros(num_classes, dtype=int))\n",
        "\n",
        "        # Count word occurrences for each class\n",
        "        for doc, label in zip(docs, self.labels):\n",
        "            for word in doc:\n",
        "                if word in self.vocabulary:\n",
        "                    word_counts[word][label] += 1\n",
        "\n",
        "        # Calculate Gini index for each word\n",
        "        gini_scores = {}\n",
        "        for word, counts in word_counts.items():\n",
        "            total = np.sum(counts)  # Total occurrences of the word across all classes\n",
        "            if total > 0:\n",
        "                probabilities = counts / total\n",
        "                gini = 1 - np.sum(probabilities ** 2)  # Gini index formula\n",
        "                gini_scores[word] = gini\n",
        "\n",
        "        self.gini_scores = gini_scores\n",
        "\n",
        "        print(f\"Initial Vocabulary Size: {len(self.vocabulary)}\")\n",
        "\n",
        "        # Filter the vocabulary based on the Gini index threshold\n",
        "        filtered_words = [word for word, gini in gini_scores.items() if gini < self.threshold]\n",
        "\n",
        "        # Rebuild the vocabulary with new contiguous indices\n",
        "        self.vocabulary = {word: idx for idx, word in enumerate(filtered_words)}\n",
        "\n",
        "        print(f\"Filtered Vocabulary Size after gini index: {len(self.vocabulary)}\")\n",
        "\n",
        "\n",
        "    def vectorize(self, doc):\n",
        "        \"\"\"Converts a preprocessed document into a Bag-of-Words vector.\"\"\"\n",
        "\n",
        "        vector = np.zeros(len(self.vocabulary))\n",
        "        for word in doc:\n",
        "            if word in self.vocabulary:\n",
        "                vector[self.vocabulary[word]] += 1\n",
        "        return vector\n",
        "\n",
        "    def preprocess_and_vectorize(self, tokenized_docs):\n",
        "        \"\"\"Preprocesses and vectorizes a list of tokenized documents.\"\"\"\n",
        "\n",
        "        preprocessed_docs = [self.preprocess(doc) for doc in tokenized_docs]\n",
        "\n",
        "        if not self.vocabulary:\n",
        "            # Build the vocabulary once\n",
        "            self.build_vocabulary(preprocessed_docs)\n",
        "            self.calculate_gini_and_filter(preprocessed_docs)\n",
        "\n",
        "        # Vectorize documents efficiently\n",
        "        vectors = np.array([self.vectorize(doc) for doc in preprocessed_docs])\n",
        "\n",
        "        return vectors, preprocessed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdUYHwclVpxg"
      },
      "source": [
        "# Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y89aSfawVpxg"
      },
      "source": [
        "## Class definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCzUzY-7Vpxg"
      },
      "outputs": [],
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Multinomial Naive Bayes classifier.\n",
        "        alpha: Laplace smoothing parameter.\n",
        "        \"\"\"\n",
        "        self.class_log_prior = {}  # Log prior probabilities log(P(class))\n",
        "        self.word_log_probs = {}   # Log conditional probabilities log(P(word | class))\n",
        "        self.vocabulary = {}       # Vocabulary from the preprocessor (word -> index)\n",
        "        self.classes = []          # Class labels\n",
        "\n",
        "    def fit(self, X, y, vocabulary):\n",
        "        \"\"\"\n",
        "        Train the Naive Bayes classifier using the provided vocabulary.\n",
        "        Parameters:\n",
        "        X (np.ndarray): Array of document vectors (Bag-of-Words representation)\n",
        "        y (list of int): Class labels\n",
        "        vocabulary (dict): Vocabulary from the preprocessor (word -> index)\n",
        "        \"\"\"\n",
        "        self.vocabulary = vocabulary  # Use the preprocessor's vocabulary\n",
        "        n_docs, vocab_size = X.shape\n",
        "        self.classes = np.unique(y)  # Find unique class labels\n",
        "\n",
        "        # Calculate log class priors log(P(class))\n",
        "        for cls in self.classes:\n",
        "            class_count = np.sum(y == cls)\n",
        "            self.class_log_prior[cls] = np.log(class_count / n_docs)\n",
        "\n",
        "        # Initialize word counts and total word counts per class\n",
        "        word_counts = {cls: np.zeros(vocab_size) for cls in self.classes}\n",
        "        total_words = {cls: 0 for cls in self.classes}\n",
        "\n",
        "        # Count word occurrences for each class\n",
        "        for doc, label in zip(X, y):\n",
        "            word_counts[label] += doc  # Add word frequencies to class counts\n",
        "            total_words[label] += np.sum(doc)  # Sum of words in the document\n",
        "\n",
        "        # Calculate log conditional probabilities log(P(word | class)) with Laplace smoothing\n",
        "        self.word_log_probs = {cls: np.zeros(vocab_size) for cls in self.classes}\n",
        "        for cls in self.classes:\n",
        "            self.word_log_probs[cls] = np.log(\n",
        "                (word_counts[cls] + 1) /\n",
        "                (total_words[cls] + vocab_size)\n",
        "            )\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for the given document vectors.\n",
        "        Parameters:\n",
        "        X (np.ndarray): Array of document vectors (Bag-of-Words representation)\n",
        "        Returns:\n",
        "        list of int: Predicted class labels\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for doc in X:\n",
        "            class_scores = {}\n",
        "            for cls in self.classes:\n",
        "                # Start with the log prior log(P(class))\n",
        "                score = self.class_log_prior[cls]\n",
        "\n",
        "                # Add log(P(word | class)) * count(word) for each word in the document\n",
        "                score += np.sum(doc * self.word_log_probs[cls])\n",
        "\n",
        "                class_scores[cls] = score\n",
        "\n",
        "            # Predict the class with the highest score\n",
        "            predictions.append(max(class_scores, key=class_scores.get))\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-QqUdzeVpxh"
      },
      "source": [
        "## Train & Validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ToMT2irVpxh"
      },
      "outputs": [],
      "source": [
        "def train_val_split(documents, labels, val_ratio=0.2, random_seed=None):\n",
        "    \"\"\"\n",
        "    Custom function to split documents and labels into training and validation sets.\n",
        "\n",
        "    Parameters:\n",
        "    - documents (list of list of str): List of documents.\n",
        "    - labels (list of int): List of corresponding labels.\n",
        "    - val_ratio (float): Fraction of data to be used as the validation set (default 0.2).\n",
        "    - random_seed (int): Random seed for reproducibility (default None).\n",
        "\n",
        "    Returns:\n",
        "    - train_docs (list of list of str): Training documents.\n",
        "    - val_docs (list of list of str): Validation documents.\n",
        "    - train_labels (list of int): Training labels.\n",
        "    - val_labels (list of int): Validation labels.\n",
        "    \"\"\"\n",
        "    # Set the random seed for reproducibility\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # Convert documents and labels to numpy arrays\n",
        "    documents = np.array(documents, dtype=object)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Shuffle the indices randomly\n",
        "    shuffled_indices = np.random.permutation(len(documents))\n",
        "\n",
        "    # Calculate the split index for the validation set\n",
        "    val_size = int(len(documents) * val_ratio)\n",
        "    val_indices = shuffled_indices[:val_size]\n",
        "    train_indices = shuffled_indices[val_size:]\n",
        "\n",
        "    # Split the documents and labels based on the indices\n",
        "    train_docs, val_docs = documents[train_indices], documents[val_indices]\n",
        "    train_labels, val_labels = labels[train_indices], labels[val_indices]\n",
        "\n",
        "    # Return the splits as lists\n",
        "    return train_docs.tolist(), val_docs.tolist(), train_labels.tolist(), val_labels.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqaDXpLtVpxh"
      },
      "source": [
        "### Split data into test and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyu1LoI9Vpxh",
        "outputId": "503b7570-bd64-4a67-cf03-1244b5e8ed66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 9052\n",
            "Validation set size: 2262\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into 80% tarining and 20% validation\n",
        "train_docs, val_docs, train_labels, val_labels = train_val_split(newsgroups_train.data, binary_labels, val_ratio=0.2, random_seed=42)\n",
        "\n",
        "# Display sizes of the splits to verify\n",
        "print(f\"Training set size: {len(train_docs)}\")\n",
        "print(f\"Validation set size: {len(val_docs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9y_EiKuVpxi"
      },
      "source": [
        "## Preprocessing the train and validation data for multinomial naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCtGnp9jVpxi",
        "outputId": "514694de-6223-429c-cbd6-1b10daed51c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: 88265 words\n",
            "Initial Vocabulary Size: 88265\n",
            "Filtered Vocabulary Size after gini index: 79839\n"
          ]
        }
      ],
      "source": [
        "# Initialize the TextPreprocessor with the training labels and a Gini index threshold of 0.3\n",
        "preprocessor = TextPreprocessor(labels=train_labels, threshold=0.3)\n",
        "\n",
        "# Tokenize documents using NLTK\n",
        "tokenized_docs = [word_tokenize(doc) for doc in train_docs]\n",
        "tokenized_val_docs = [word_tokenize(doc) for doc in val_docs]\n",
        "\n",
        "# Preprocess and vectorize the tokenized documents\n",
        "doc_vector, preprocessed_docs = preprocessor.preprocess_and_vectorize(tokenized_docs)\n",
        "\n",
        "# Preprocess and vectorize the tokenized validation documents\n",
        "val_vectors, preprocessed_val_docs = preprocessor.preprocess_and_vectorize(tokenized_val_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb95VzX-Vpxi"
      },
      "source": [
        "## Training the multinomial naive bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5vXSpiTuVpxi"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the Multinomial Naive Bayes classifier\n",
        "nb = MultinomialNaiveBayes()\n",
        "nb.fit(doc_vector, np.array(train_labels), preprocessor.vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KmoIbhyVpxj"
      },
      "source": [
        "## Evaluation functions for performace metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MfpW2AmIVpxj"
      },
      "outputs": [],
      "source": [
        "def evaluate_metrics(predictions, labels):\n",
        "    \"\"\"Evaluates accuracy, precision, recall, and F1-score.\"\"\"\n",
        "    correct_predictions = sum([1 for true, pred in zip(labels, predictions) if true == pred])\n",
        "    accuracy = correct_predictions / len(labels)\n",
        "\n",
        "    # Initialize counters for Precision, Recall, and F1-score calculations\n",
        "    true_positive = false_positive = false_negative = 0\n",
        "\n",
        "    for true, pred in zip(labels, predictions):\n",
        "        if pred == 1:\n",
        "            if true == 1:\n",
        "                true_positive += 1\n",
        "            else:\n",
        "                false_positive += 1\n",
        "        elif true == 1:\n",
        "            false_negative += 1\n",
        "\n",
        "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
        "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyMFO-eAVpxj"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ym2u-aVzVpxj",
        "outputId": "49d3808f-61df-4cc5-d75f-c921978b3c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********VALIDATION METRICS FOR NAIVE BAYES CLASSIFIER**********\n",
            "\n",
            "Accuracy: 0.8948\n",
            "Precision: 0.8768\n",
            "Recall: 0.9336\n",
            "F1-Score: 0.9043\n"
          ]
        }
      ],
      "source": [
        "# Predict the validation set labels\n",
        "val_predictions = nb.predict(val_vectors)\n",
        "\n",
        "# Evaluate the classifier's performance\n",
        "accuracy, precision, recall, f1 = evaluate_metrics(val_predictions, val_labels)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"*\"*10 + \"VALIDATION METRICS FOR NAIVE BAYES CLASSIFIER\"+ \"*\"*10 +\"\\n\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbhO4AkFVpxj"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWGxtBrLVpxj"
      },
      "source": [
        "## Class definition for the Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ynLQsyQmVpxj"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000, reqularization=0.01, tolerance=1e-5, patience=5):\n",
        "        \"\"\"\n",
        "        Initialize the Logistic Regression model.\n",
        "        learning_rate: Step size for gradient descent.\n",
        "        \"\"\"\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.regularization = reqularization\n",
        "        self.tolerance = tolerance\n",
        "        self.patience = patience\n",
        "\n",
        "        self.weights = None\n",
        "        self.losses = []\n",
        "\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"Apply the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
        "\n",
        "        # Ensure inputs are NumPy arrays\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "        epsilon = 1e-9  # To prevent log(0)\n",
        "\n",
        "        return -np.mean(y_true * np.log(y_pred + epsilon) +\n",
        "                        (1 - y_true) * np.log(1 - y_pred + epsilon)) + self.regularization * 0.5 * np.sum(self.weights ** 2)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the Logistic Regression model using gradient descent with convergence check.\n",
        "        X: Document-term matrix (shape: [n_samples, n_features]).\n",
        "        y: Target labels (shape: [n_samples]).\n",
        "        tol: Tolerance for convergence (default: 1e-6).\n",
        "        patience: Number of iterations with minimal improvement before stopping early.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(1 + n_features)\n",
        "\n",
        "        # Add bias term to the input data (column of ones)\n",
        "        X = np.c_[np.ones((n_samples, 1)), X]\n",
        "\n",
        "        prev_loss = float('inf')\n",
        "        no_improvement = 0\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            Z = np.dot(X, self.weights)\n",
        "            y_pred = self._sigmoid(Z)\n",
        "\n",
        "            # Compute the current loss\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "\n",
        "            # Print progress every 50 iterations\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Iteration {i} - Loss: {loss:.6f}\")\n",
        "                self.losses.append((i,loss))\n",
        "\n",
        "            # Check for convergence (loss change < tolerance)\n",
        "            if abs(prev_loss - loss) < self.tolerance:\n",
        "                no_improvement += 1  # Increment if improvement is minimal\n",
        "                if no_improvement >= self.patience:\n",
        "                    print(f\"Convergence achieved after {i} iterations.\")\n",
        "                    break  # Stop early if convergence is detected\n",
        "            else:\n",
        "                no_improvement = 0  # Reset if improvement is significant\n",
        "\n",
        "            # Store the current loss as the previous loss for the next iteration\n",
        "            prev_loss = loss\n",
        "\n",
        "            # Compute gradients\n",
        "            gradient = np.dot(X.T, (y_pred - y)) / n_samples + self.regularization * self.weights\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.lr * gradient\n",
        "\n",
        "        print(f\"Training completed in {i} iterations.\")\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for the given input data.\n",
        "        X: Document-term matrix (shape: [n_samples, n_features]).\n",
        "        Returns:\n",
        "        np.ndarray: Predicted binary class labels (0 or 1).\n",
        "        \"\"\"\n",
        "        # Add bias in the input data\n",
        "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "        linear_model = np.dot(X, self.weights)\n",
        "        y_pred = self._sigmoid(linear_model)\n",
        "\n",
        "        return (y_pred > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrss3gIaVpxk"
      },
      "source": [
        "## Preprocessing the train and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SizZlmJVpxk"
      },
      "outputs": [],
      "source": [
        "# Assume you have your training and test datasets\n",
        "X_train, y_train = train_docs, np.array(train_labels)\n",
        "X_val, y_val = val_docs, np.array(val_labels)\n",
        "\n",
        "# Initialize the preprocessor\n",
        "logistic_preprocessor = TextPreprocessor(labels=y_train, threshold=0.3)\n",
        "\n",
        "# Tokenize the training and test datasets\n",
        "tokenized_train_docs = [word_tokenize(doc) for doc in X_train]\n",
        "tokenized_val_docs = [word_tokenize(doc) for doc in X_val]\n",
        "\n",
        "# Preprocess and vectorize the training and test datasets\n",
        "X_train_vectors, _ = logistic_preprocessor.preprocess_and_vectorize(tokenized_train_docs)\n",
        "X_val_vectors, _ = logistic_preprocessor.preprocess_and_vectorize(tokenized_val_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BymqW9TyVpxk"
      },
      "source": [
        "## Training the model with different hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2l_HZTwVpxk"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(learning_rate=0.1, n_iters=1000, reqularization=0.01, tolerance=1e-5, patience=3)\n",
        "model.fit(X_train_vectors, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIELnxdKVpxk"
      },
      "source": [
        "## Validation of the logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB-SRJqGVpxk"
      },
      "outputs": [],
      "source": [
        "# Predict on the validation set\n",
        "y_pred = model.predict(X_val_vectors)\n",
        "\n",
        "# Calculate the metrics\n",
        "accuracy, precision, recall, f1 = evaluate_metrics(y_pred, y_val)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"*\"*10 + \"VALIDATION METRICS FOR LOGISTIC REGRESSION CLASSIFIER\"+ \"*\"*10 +\"\\n\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98r5I7fhVpxk"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EL70xUGVpxk"
      },
      "source": [
        "## Getting the test data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x2T8YgGVpxk"
      },
      "outputs": [],
      "source": [
        "# pull the test data from the dataset\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=group_1 + group_2, remove=('headers', 'footers', 'quotes'))\n",
        "test_documents = newsgroups_test.data\n",
        "\n",
        "# Relabel the target: 0 for Group 1, 1 for Group 2\n",
        "group_1_labels = [newsgroups_test.target_names.index(cat) for cat in group_1]\n",
        "group_2_labels = [newsgroups_test.target_names.index(cat) for cat in group_2]\n",
        "\n",
        "test_binary_labels = [0 if label in group_1_labels else 1 for label in newsgroups_test.target]\n",
        "\n",
        "print(f'Number of test data points: {len(newsgroups_test.data)}')\n",
        "print(f'First 500 class labels: {test_binary_labels[:500]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDefSHcRVpxl"
      },
      "source": [
        "## Testing Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkUGXNzvVpxl"
      },
      "outputs": [],
      "source": [
        "# Tokenize the test documents\n",
        "tokenized_test_docs = [word_tokenize(doc) for doc in test_documents]\n",
        "\n",
        "# Preprocess and vectorize the tokenized test documents\n",
        "test_doc_vector, preprocessed_test_docs = preprocessor.preprocess_and_vectorize(tokenized_test_docs)\n",
        "\n",
        "# Predict the test set labels using the Naive Bayes classifier\n",
        "predictions = nb.predict(test_doc_vector)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy, precision, recall, f1 = evaluate_metrics(predictions, test_binary_labels)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"*\"*10 + \"TEST METRICS FOR NAIVE BAYES CLASSIFIER\"+ \"*\"*10 +\"\\n\")\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSOmuZB4Vpxl"
      },
      "source": [
        "## Testing the logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cZpwB97Vpxl"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set using the Logistic Regression model\n",
        "predictions = model.predict(test_doc_vector)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy, precision, recall, f1 = evaluate_metrics(predictions, test_binary_labels)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"*\"*10 + \"TEST METRICS FOR LOGISTIC REGRESSION CLASSIFIER\"+ \"*\"*10 +\"\\n\")\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1 Score: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o__ESBzRVpxl"
      },
      "source": [
        "# Miscellaneous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyETSy04Vpxl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the loss values stored during training\n",
        "iterations, losses = zip(*model.losses)  # Assuming model.losses stores (iteration, loss)\n",
        "plt.plot(iterations, losses)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(f\"Loss vs Iterations [ learning_rate={model.lr}, \\n regularization={model.regularization}, tolerance={model.tolerance}, patience={model.patience} ]\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}